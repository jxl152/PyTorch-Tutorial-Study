{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deep_residual_network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMKyxIsrOB8FlZKII2LX0OG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ZKKhyxF8Wt0Q","executionInfo":{"status":"ok","timestamp":1658502354586,"user_tz":-120,"elapsed":3177,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}}},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","source":["# 1. Hyper-parameters and Dataset"],"metadata":{"id":"sIbq0KwnJ6jJ"}},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","num_epochs = 80\n","batch_size = 100\n","learning_rate = 0.001"],"metadata":{"id":"VCgv7reFW1Fy","executionInfo":{"status":"ok","timestamp":1658502354587,"user_tz":-120,"elapsed":6,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIhBOTt4W4Ph","executionInfo":{"status":"ok","timestamp":1658502375524,"user_tz":-120,"elapsed":20942,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"75613d85-c0d1-4cd0-a2ec-3ba5e4db7275"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## 1.1 Image preprocessing\n","\n","**According to the paper:**  [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).\n","1. We follow the simple data augmentation for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip. \n","2. For testing, we only evaluate the single view of the original 32×32 image."],"metadata":{"id":"lQukBxnUaiTe"}},{"cell_type":"code","source":["data_dir = '/content/drive/My Drive/PyTorch/Github_Series/02-intermediate/'\n","\n","# Image preprocessing modules\n","transform = transforms.Compose([\n","  transforms.Pad(padding=4),\n","  transforms.RandomHorizontalFlip(p=0.5),\n","  transforms.RandomCrop(size=32),\n","  transforms.ToTensor()])\n","\n","# CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root=data_dir,\n","                                             train=True, \n","                                             transform=transform,\n","                                             download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root=data_dir,\n","                                            train=False, \n","                                            transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-66_OAEYtDC","executionInfo":{"status":"ok","timestamp":1658502384110,"user_tz":-120,"elapsed":8589,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"7ddb1801-418d-4ae8-d0af-0352794067d9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# 2. Modeling and Training\n","\n","1. The architecture is illustrated in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf). "],"metadata":{"id":"aOkZn3pwKK9h"}},{"cell_type":"code","source":["# 3x3 convolution\n","def conv3x3(in_channels, out_channels, stride=1):\n","  # padding=1 for keeping the input and output of the same dimensions\n","  return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n","                   stride=stride, padding=1, bias=False)\n","\n","# 1x1 convolution: used to match dimensions\n","def conv1x1(in_channels, out_channels, stride=1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=1, \n","                     stride=stride, bias=False)\n","\n","# Residual block\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","      super().__init__()\n","      self.conv1 = conv3x3(in_channels, out_channels, stride)\n","      # We adopt batch normalization (BN) right after each convolution and before activation\n","      self.bn1 = nn.BatchNorm2d(out_channels)\n","      self.relu = nn.ReLU(inplace=True)\n","      self.conv2 = conv3x3(out_channels, out_channels)\n","      self.bn2 = nn.BatchNorm2d(out_channels)\n","      self.downsample = downsample\n","        \n","    def forward(self, x):\n","      residual = x\n","      out = self.conv1(x)\n","      out = self.bn1(out)\n","      out = self.relu(out)\n","      out = self.conv2(out)\n","      out = self.bn2(out)\n","      if self.downsample:\n","        residual = self.downsample(x)\n","      out += residual\n","      # We adopt the second nonlinearity after the addition, as shown in Figure 5 in the paper.\n","      out = self.relu(out)\n","      return out\n","\n","# ResNet\n","class ResNet(nn.Module):\n","    def __init__(self, layers, num_classes=10):\n","      super().__init__()\n","      self.conv = conv3x3(3, 16)\n","      self.bn = nn.BatchNorm2d(16)\n","      self.relu = nn.ReLU(inplace=True)\n","      # Each layer consists of several Residual blocks\n","      self.layer1 = self.__make_layer(16, 16, layers[0])\n","      self.layer2 = self.__make_layer(16, 32, layers[1], 2)  # stride=2\n","      self.layer3 = self.__make_layer(32, 64, layers[2], 2)\n","      self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","      self.fc = nn.Linear(64, num_classes)\n","\n","    def __make_layer(self, in_channels, out_channels, layer_num, stride=1):\n","      downsample = None\n","      if (stride != 1) or (in_channels != out_channels):\n","        downsample = nn.Sequential(\n","            conv1x1(in_channels, out_channels, stride),\n","            nn.BatchNorm2d(out_channels))\n","      \n","      layers = []\n","      layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n","      for _ in range(1, layer_num):\n","        layers.append(ResidualBlock(out_channels, out_channels))\n","      return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","      out = self.conv(x)\n","      out = self.bn(out)\n","      out = self.relu(out)\n","      out = self.layer1(out)\n","      out = self.layer2(out)\n","      out = self.layer3(out)\n","      out = self.avg_pool(out)\n","      out = torch.flatten(out, start_dim=1)\n","      out = self.fc(out)\n","      return out"],"metadata":{"id":"AbZ_0t6tYvQC","executionInfo":{"status":"ok","timestamp":1658502384112,"user_tz":-120,"elapsed":7,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Initialize the deep residual model\n","model = ResNet([2, 2, 2]).to(device)\n","\n","# loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","def update_lr(optimizer, lr):\n","  for param_group in optimizer.param_groups:\n","    param_group['lr'] = lr\n","\n","# Train the model\n","total_step = len(train_loader)\n","curr_lr = learning_rate\n","for epoch in range(num_epochs):\n","  for batch_id, (images, labels) in enumerate(train_loader):\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Feedward\n","    output = model(images)\n","    loss = loss_fn(output, labels)\n","\n","    # Backward propagation\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (batch_id+1) % 100 == 0:\n","      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n","            .format(epoch+1, num_epochs, batch_id+1, total_step, loss.item()))\n","  \n","  # decay learning rate\n","  if (epoch+1) % 20 == 0:\n","    curr_lr /= 3\n","    update_lr(optimizer, curr_lr)"],"metadata":{"id":"TzGCWRShD0w1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658503806989,"user_tz":-120,"elapsed":1422882,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"b6958da2-1ae3-469b-e389-80b8f0f614d8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: [1/80], Step: [100/500], Loss: 1.6191\n","Epoch: [1/80], Step: [200/500], Loss: 1.3692\n","Epoch: [1/80], Step: [300/500], Loss: 1.3477\n","Epoch: [1/80], Step: [400/500], Loss: 1.1827\n","Epoch: [1/80], Step: [500/500], Loss: 1.0481\n","Epoch: [2/80], Step: [100/500], Loss: 1.2261\n","Epoch: [2/80], Step: [200/500], Loss: 1.1172\n","Epoch: [2/80], Step: [300/500], Loss: 1.0015\n","Epoch: [2/80], Step: [400/500], Loss: 0.9061\n","Epoch: [2/80], Step: [500/500], Loss: 0.9209\n","Epoch: [3/80], Step: [100/500], Loss: 0.9845\n","Epoch: [3/80], Step: [200/500], Loss: 0.9232\n","Epoch: [3/80], Step: [300/500], Loss: 0.8227\n","Epoch: [3/80], Step: [400/500], Loss: 0.7213\n","Epoch: [3/80], Step: [500/500], Loss: 0.8097\n","Epoch: [4/80], Step: [100/500], Loss: 0.6373\n","Epoch: [4/80], Step: [200/500], Loss: 0.9143\n","Epoch: [4/80], Step: [300/500], Loss: 0.6516\n","Epoch: [4/80], Step: [400/500], Loss: 0.6984\n","Epoch: [4/80], Step: [500/500], Loss: 0.7553\n","Epoch: [5/80], Step: [100/500], Loss: 0.6536\n","Epoch: [5/80], Step: [200/500], Loss: 0.8221\n","Epoch: [5/80], Step: [300/500], Loss: 0.6613\n","Epoch: [5/80], Step: [400/500], Loss: 0.9192\n","Epoch: [5/80], Step: [500/500], Loss: 0.6213\n","Epoch: [6/80], Step: [100/500], Loss: 0.6626\n","Epoch: [6/80], Step: [200/500], Loss: 0.9300\n","Epoch: [6/80], Step: [300/500], Loss: 0.5659\n","Epoch: [6/80], Step: [400/500], Loss: 0.7790\n","Epoch: [6/80], Step: [500/500], Loss: 0.5305\n","Epoch: [7/80], Step: [100/500], Loss: 0.5208\n","Epoch: [7/80], Step: [200/500], Loss: 0.6151\n","Epoch: [7/80], Step: [300/500], Loss: 0.5772\n","Epoch: [7/80], Step: [400/500], Loss: 0.6125\n","Epoch: [7/80], Step: [500/500], Loss: 0.6494\n","Epoch: [8/80], Step: [100/500], Loss: 0.5231\n","Epoch: [8/80], Step: [200/500], Loss: 0.5338\n","Epoch: [8/80], Step: [300/500], Loss: 0.7509\n","Epoch: [8/80], Step: [400/500], Loss: 0.5760\n","Epoch: [8/80], Step: [500/500], Loss: 0.4303\n","Epoch: [9/80], Step: [100/500], Loss: 0.6032\n","Epoch: [9/80], Step: [200/500], Loss: 0.5848\n","Epoch: [9/80], Step: [300/500], Loss: 0.5666\n","Epoch: [9/80], Step: [400/500], Loss: 0.5939\n","Epoch: [9/80], Step: [500/500], Loss: 0.4658\n","Epoch: [10/80], Step: [100/500], Loss: 0.3937\n","Epoch: [10/80], Step: [200/500], Loss: 0.6215\n","Epoch: [10/80], Step: [300/500], Loss: 0.6335\n","Epoch: [10/80], Step: [400/500], Loss: 0.5665\n","Epoch: [10/80], Step: [500/500], Loss: 0.4930\n","Epoch: [11/80], Step: [100/500], Loss: 0.5136\n","Epoch: [11/80], Step: [200/500], Loss: 0.6316\n","Epoch: [11/80], Step: [300/500], Loss: 0.3948\n","Epoch: [11/80], Step: [400/500], Loss: 0.3679\n","Epoch: [11/80], Step: [500/500], Loss: 0.5067\n","Epoch: [12/80], Step: [100/500], Loss: 0.3265\n","Epoch: [12/80], Step: [200/500], Loss: 0.4686\n","Epoch: [12/80], Step: [300/500], Loss: 0.6894\n","Epoch: [12/80], Step: [400/500], Loss: 0.4448\n","Epoch: [12/80], Step: [500/500], Loss: 0.3791\n","Epoch: [13/80], Step: [100/500], Loss: 0.5092\n","Epoch: [13/80], Step: [200/500], Loss: 0.4254\n","Epoch: [13/80], Step: [300/500], Loss: 0.4486\n","Epoch: [13/80], Step: [400/500], Loss: 0.5912\n","Epoch: [13/80], Step: [500/500], Loss: 0.5547\n","Epoch: [14/80], Step: [100/500], Loss: 0.3922\n","Epoch: [14/80], Step: [200/500], Loss: 0.3804\n","Epoch: [14/80], Step: [300/500], Loss: 0.3534\n","Epoch: [14/80], Step: [400/500], Loss: 0.3788\n","Epoch: [14/80], Step: [500/500], Loss: 0.4385\n","Epoch: [15/80], Step: [100/500], Loss: 0.4622\n","Epoch: [15/80], Step: [200/500], Loss: 0.4751\n","Epoch: [15/80], Step: [300/500], Loss: 0.3823\n","Epoch: [15/80], Step: [400/500], Loss: 0.6453\n","Epoch: [15/80], Step: [500/500], Loss: 0.3501\n","Epoch: [16/80], Step: [100/500], Loss: 0.2797\n","Epoch: [16/80], Step: [200/500], Loss: 0.3384\n","Epoch: [16/80], Step: [300/500], Loss: 0.5377\n","Epoch: [16/80], Step: [400/500], Loss: 0.6184\n","Epoch: [16/80], Step: [500/500], Loss: 0.3332\n","Epoch: [17/80], Step: [100/500], Loss: 0.4230\n","Epoch: [17/80], Step: [200/500], Loss: 0.3920\n","Epoch: [17/80], Step: [300/500], Loss: 0.5642\n","Epoch: [17/80], Step: [400/500], Loss: 0.5136\n","Epoch: [17/80], Step: [500/500], Loss: 0.3625\n","Epoch: [18/80], Step: [100/500], Loss: 0.4017\n","Epoch: [18/80], Step: [200/500], Loss: 0.2842\n","Epoch: [18/80], Step: [300/500], Loss: 0.4066\n","Epoch: [18/80], Step: [400/500], Loss: 0.3876\n","Epoch: [18/80], Step: [500/500], Loss: 0.3920\n","Epoch: [19/80], Step: [100/500], Loss: 0.3889\n","Epoch: [19/80], Step: [200/500], Loss: 0.5653\n","Epoch: [19/80], Step: [300/500], Loss: 0.3692\n","Epoch: [19/80], Step: [400/500], Loss: 0.3906\n","Epoch: [19/80], Step: [500/500], Loss: 0.2314\n","Epoch: [20/80], Step: [100/500], Loss: 0.4567\n","Epoch: [20/80], Step: [200/500], Loss: 0.5149\n","Epoch: [20/80], Step: [300/500], Loss: 0.3634\n","Epoch: [20/80], Step: [400/500], Loss: 0.3252\n","Epoch: [20/80], Step: [500/500], Loss: 0.3077\n","Epoch: [21/80], Step: [100/500], Loss: 0.2791\n","Epoch: [21/80], Step: [200/500], Loss: 0.4693\n","Epoch: [21/80], Step: [300/500], Loss: 0.4144\n","Epoch: [21/80], Step: [400/500], Loss: 0.2769\n","Epoch: [21/80], Step: [500/500], Loss: 0.2552\n","Epoch: [22/80], Step: [100/500], Loss: 0.2948\n","Epoch: [22/80], Step: [200/500], Loss: 0.2394\n","Epoch: [22/80], Step: [300/500], Loss: 0.3183\n","Epoch: [22/80], Step: [400/500], Loss: 0.3053\n","Epoch: [22/80], Step: [500/500], Loss: 0.3671\n","Epoch: [23/80], Step: [100/500], Loss: 0.2023\n","Epoch: [23/80], Step: [200/500], Loss: 0.2696\n","Epoch: [23/80], Step: [300/500], Loss: 0.3801\n","Epoch: [23/80], Step: [400/500], Loss: 0.3198\n","Epoch: [23/80], Step: [500/500], Loss: 0.1209\n","Epoch: [24/80], Step: [100/500], Loss: 0.4761\n","Epoch: [24/80], Step: [200/500], Loss: 0.2303\n","Epoch: [24/80], Step: [300/500], Loss: 0.3530\n","Epoch: [24/80], Step: [400/500], Loss: 0.2247\n","Epoch: [24/80], Step: [500/500], Loss: 0.2606\n","Epoch: [25/80], Step: [100/500], Loss: 0.2607\n","Epoch: [25/80], Step: [200/500], Loss: 0.3693\n","Epoch: [25/80], Step: [300/500], Loss: 0.3246\n","Epoch: [25/80], Step: [400/500], Loss: 0.2724\n","Epoch: [25/80], Step: [500/500], Loss: 0.3774\n","Epoch: [26/80], Step: [100/500], Loss: 0.1666\n","Epoch: [26/80], Step: [200/500], Loss: 0.1757\n","Epoch: [26/80], Step: [300/500], Loss: 0.2578\n","Epoch: [26/80], Step: [400/500], Loss: 0.2075\n","Epoch: [26/80], Step: [500/500], Loss: 0.2348\n","Epoch: [27/80], Step: [100/500], Loss: 0.3443\n","Epoch: [27/80], Step: [200/500], Loss: 0.2278\n","Epoch: [27/80], Step: [300/500], Loss: 0.2060\n","Epoch: [27/80], Step: [400/500], Loss: 0.2560\n","Epoch: [27/80], Step: [500/500], Loss: 0.3750\n","Epoch: [28/80], Step: [100/500], Loss: 0.2521\n","Epoch: [28/80], Step: [200/500], Loss: 0.2623\n","Epoch: [28/80], Step: [300/500], Loss: 0.2221\n","Epoch: [28/80], Step: [400/500], Loss: 0.2149\n","Epoch: [28/80], Step: [500/500], Loss: 0.2598\n","Epoch: [29/80], Step: [100/500], Loss: 0.2018\n","Epoch: [29/80], Step: [200/500], Loss: 0.2371\n","Epoch: [29/80], Step: [300/500], Loss: 0.2095\n","Epoch: [29/80], Step: [400/500], Loss: 0.2725\n","Epoch: [29/80], Step: [500/500], Loss: 0.3071\n","Epoch: [30/80], Step: [100/500], Loss: 0.1832\n","Epoch: [30/80], Step: [200/500], Loss: 0.2630\n","Epoch: [30/80], Step: [300/500], Loss: 0.2854\n","Epoch: [30/80], Step: [400/500], Loss: 0.1955\n","Epoch: [30/80], Step: [500/500], Loss: 0.1932\n","Epoch: [31/80], Step: [100/500], Loss: 0.3192\n","Epoch: [31/80], Step: [200/500], Loss: 0.2786\n","Epoch: [31/80], Step: [300/500], Loss: 0.2897\n","Epoch: [31/80], Step: [400/500], Loss: 0.2325\n","Epoch: [31/80], Step: [500/500], Loss: 0.3886\n","Epoch: [32/80], Step: [100/500], Loss: 0.3540\n","Epoch: [32/80], Step: [200/500], Loss: 0.2816\n","Epoch: [32/80], Step: [300/500], Loss: 0.1805\n","Epoch: [32/80], Step: [400/500], Loss: 0.3064\n","Epoch: [32/80], Step: [500/500], Loss: 0.2344\n","Epoch: [33/80], Step: [100/500], Loss: 0.1815\n","Epoch: [33/80], Step: [200/500], Loss: 0.1217\n","Epoch: [33/80], Step: [300/500], Loss: 0.3073\n","Epoch: [33/80], Step: [400/500], Loss: 0.3209\n","Epoch: [33/80], Step: [500/500], Loss: 0.2818\n","Epoch: [34/80], Step: [100/500], Loss: 0.2876\n","Epoch: [34/80], Step: [200/500], Loss: 0.1582\n","Epoch: [34/80], Step: [300/500], Loss: 0.1887\n","Epoch: [34/80], Step: [400/500], Loss: 0.2504\n","Epoch: [34/80], Step: [500/500], Loss: 0.2803\n","Epoch: [35/80], Step: [100/500], Loss: 0.2740\n","Epoch: [35/80], Step: [200/500], Loss: 0.2178\n","Epoch: [35/80], Step: [300/500], Loss: 0.2162\n","Epoch: [35/80], Step: [400/500], Loss: 0.2280\n","Epoch: [35/80], Step: [500/500], Loss: 0.2138\n","Epoch: [36/80], Step: [100/500], Loss: 0.2695\n","Epoch: [36/80], Step: [200/500], Loss: 0.2824\n","Epoch: [36/80], Step: [300/500], Loss: 0.2990\n","Epoch: [36/80], Step: [400/500], Loss: 0.2699\n","Epoch: [36/80], Step: [500/500], Loss: 0.2639\n","Epoch: [37/80], Step: [100/500], Loss: 0.2508\n","Epoch: [37/80], Step: [200/500], Loss: 0.2917\n","Epoch: [37/80], Step: [300/500], Loss: 0.2208\n","Epoch: [37/80], Step: [400/500], Loss: 0.2235\n","Epoch: [37/80], Step: [500/500], Loss: 0.2333\n","Epoch: [38/80], Step: [100/500], Loss: 0.1629\n","Epoch: [38/80], Step: [200/500], Loss: 0.3060\n","Epoch: [38/80], Step: [300/500], Loss: 0.4398\n","Epoch: [38/80], Step: [400/500], Loss: 0.2627\n","Epoch: [38/80], Step: [500/500], Loss: 0.2451\n","Epoch: [39/80], Step: [100/500], Loss: 0.2421\n","Epoch: [39/80], Step: [200/500], Loss: 0.3423\n","Epoch: [39/80], Step: [300/500], Loss: 0.2732\n","Epoch: [39/80], Step: [400/500], Loss: 0.3005\n","Epoch: [39/80], Step: [500/500], Loss: 0.2498\n","Epoch: [40/80], Step: [100/500], Loss: 0.2352\n","Epoch: [40/80], Step: [200/500], Loss: 0.2503\n","Epoch: [40/80], Step: [300/500], Loss: 0.2872\n","Epoch: [40/80], Step: [400/500], Loss: 0.3677\n","Epoch: [40/80], Step: [500/500], Loss: 0.2325\n","Epoch: [41/80], Step: [100/500], Loss: 0.2070\n","Epoch: [41/80], Step: [200/500], Loss: 0.2596\n","Epoch: [41/80], Step: [300/500], Loss: 0.1951\n","Epoch: [41/80], Step: [400/500], Loss: 0.2597\n","Epoch: [41/80], Step: [500/500], Loss: 0.2562\n","Epoch: [42/80], Step: [100/500], Loss: 0.3121\n","Epoch: [42/80], Step: [200/500], Loss: 0.2127\n","Epoch: [42/80], Step: [300/500], Loss: 0.1755\n","Epoch: [42/80], Step: [400/500], Loss: 0.3373\n","Epoch: [42/80], Step: [500/500], Loss: 0.2856\n","Epoch: [43/80], Step: [100/500], Loss: 0.2330\n","Epoch: [43/80], Step: [200/500], Loss: 0.2054\n","Epoch: [43/80], Step: [300/500], Loss: 0.2493\n","Epoch: [43/80], Step: [400/500], Loss: 0.1985\n","Epoch: [43/80], Step: [500/500], Loss: 0.1702\n","Epoch: [44/80], Step: [100/500], Loss: 0.1405\n","Epoch: [44/80], Step: [200/500], Loss: 0.2379\n","Epoch: [44/80], Step: [300/500], Loss: 0.1239\n","Epoch: [44/80], Step: [400/500], Loss: 0.0936\n","Epoch: [44/80], Step: [500/500], Loss: 0.1682\n","Epoch: [45/80], Step: [100/500], Loss: 0.1543\n","Epoch: [45/80], Step: [200/500], Loss: 0.1610\n","Epoch: [45/80], Step: [300/500], Loss: 0.2606\n","Epoch: [45/80], Step: [400/500], Loss: 0.2193\n","Epoch: [45/80], Step: [500/500], Loss: 0.2417\n","Epoch: [46/80], Step: [100/500], Loss: 0.1728\n","Epoch: [46/80], Step: [200/500], Loss: 0.2353\n","Epoch: [46/80], Step: [300/500], Loss: 0.3179\n","Epoch: [46/80], Step: [400/500], Loss: 0.2445\n","Epoch: [46/80], Step: [500/500], Loss: 0.1932\n","Epoch: [47/80], Step: [100/500], Loss: 0.1803\n","Epoch: [47/80], Step: [200/500], Loss: 0.0957\n","Epoch: [47/80], Step: [300/500], Loss: 0.2305\n","Epoch: [47/80], Step: [400/500], Loss: 0.1846\n","Epoch: [47/80], Step: [500/500], Loss: 0.2155\n","Epoch: [48/80], Step: [100/500], Loss: 0.1670\n","Epoch: [48/80], Step: [200/500], Loss: 0.1523\n","Epoch: [48/80], Step: [300/500], Loss: 0.2388\n","Epoch: [48/80], Step: [400/500], Loss: 0.1938\n","Epoch: [48/80], Step: [500/500], Loss: 0.1892\n","Epoch: [49/80], Step: [100/500], Loss: 0.2053\n","Epoch: [49/80], Step: [200/500], Loss: 0.1592\n","Epoch: [49/80], Step: [300/500], Loss: 0.2139\n","Epoch: [49/80], Step: [400/500], Loss: 0.1743\n","Epoch: [49/80], Step: [500/500], Loss: 0.3443\n","Epoch: [50/80], Step: [100/500], Loss: 0.1755\n","Epoch: [50/80], Step: [200/500], Loss: 0.1903\n","Epoch: [50/80], Step: [300/500], Loss: 0.1819\n","Epoch: [50/80], Step: [400/500], Loss: 0.2220\n","Epoch: [50/80], Step: [500/500], Loss: 0.2001\n","Epoch: [51/80], Step: [100/500], Loss: 0.1420\n","Epoch: [51/80], Step: [200/500], Loss: 0.1749\n","Epoch: [51/80], Step: [300/500], Loss: 0.1214\n","Epoch: [51/80], Step: [400/500], Loss: 0.1288\n","Epoch: [51/80], Step: [500/500], Loss: 0.1192\n","Epoch: [52/80], Step: [100/500], Loss: 0.1229\n","Epoch: [52/80], Step: [200/500], Loss: 0.2819\n","Epoch: [52/80], Step: [300/500], Loss: 0.1926\n","Epoch: [52/80], Step: [400/500], Loss: 0.1108\n","Epoch: [52/80], Step: [500/500], Loss: 0.1574\n","Epoch: [53/80], Step: [100/500], Loss: 0.2244\n","Epoch: [53/80], Step: [200/500], Loss: 0.1782\n","Epoch: [53/80], Step: [300/500], Loss: 0.1814\n","Epoch: [53/80], Step: [400/500], Loss: 0.1261\n","Epoch: [53/80], Step: [500/500], Loss: 0.1165\n","Epoch: [54/80], Step: [100/500], Loss: 0.2006\n","Epoch: [54/80], Step: [200/500], Loss: 0.1954\n","Epoch: [54/80], Step: [300/500], Loss: 0.0781\n","Epoch: [54/80], Step: [400/500], Loss: 0.2573\n","Epoch: [54/80], Step: [500/500], Loss: 0.0793\n","Epoch: [55/80], Step: [100/500], Loss: 0.3089\n","Epoch: [55/80], Step: [200/500], Loss: 0.1085\n","Epoch: [55/80], Step: [300/500], Loss: 0.2088\n","Epoch: [55/80], Step: [400/500], Loss: 0.2168\n","Epoch: [55/80], Step: [500/500], Loss: 0.1424\n","Epoch: [56/80], Step: [100/500], Loss: 0.1449\n","Epoch: [56/80], Step: [200/500], Loss: 0.1751\n","Epoch: [56/80], Step: [300/500], Loss: 0.3365\n","Epoch: [56/80], Step: [400/500], Loss: 0.2522\n","Epoch: [56/80], Step: [500/500], Loss: 0.1687\n","Epoch: [57/80], Step: [100/500], Loss: 0.3010\n","Epoch: [57/80], Step: [200/500], Loss: 0.2900\n","Epoch: [57/80], Step: [300/500], Loss: 0.2329\n","Epoch: [57/80], Step: [400/500], Loss: 0.1552\n","Epoch: [57/80], Step: [500/500], Loss: 0.2744\n","Epoch: [58/80], Step: [100/500], Loss: 0.0919\n","Epoch: [58/80], Step: [200/500], Loss: 0.1632\n","Epoch: [58/80], Step: [300/500], Loss: 0.3099\n","Epoch: [58/80], Step: [400/500], Loss: 0.2064\n","Epoch: [58/80], Step: [500/500], Loss: 0.1297\n","Epoch: [59/80], Step: [100/500], Loss: 0.1793\n","Epoch: [59/80], Step: [200/500], Loss: 0.1511\n","Epoch: [59/80], Step: [300/500], Loss: 0.1310\n","Epoch: [59/80], Step: [400/500], Loss: 0.2364\n","Epoch: [59/80], Step: [500/500], Loss: 0.1047\n","Epoch: [60/80], Step: [100/500], Loss: 0.1005\n","Epoch: [60/80], Step: [200/500], Loss: 0.1504\n","Epoch: [60/80], Step: [300/500], Loss: 0.2484\n","Epoch: [60/80], Step: [400/500], Loss: 0.1078\n","Epoch: [60/80], Step: [500/500], Loss: 0.1680\n","Epoch: [61/80], Step: [100/500], Loss: 0.1519\n","Epoch: [61/80], Step: [200/500], Loss: 0.1860\n","Epoch: [61/80], Step: [300/500], Loss: 0.1220\n","Epoch: [61/80], Step: [400/500], Loss: 0.1596\n","Epoch: [61/80], Step: [500/500], Loss: 0.1433\n","Epoch: [62/80], Step: [100/500], Loss: 0.2478\n","Epoch: [62/80], Step: [200/500], Loss: 0.2295\n","Epoch: [62/80], Step: [300/500], Loss: 0.1509\n","Epoch: [62/80], Step: [400/500], Loss: 0.1913\n","Epoch: [62/80], Step: [500/500], Loss: 0.1375\n","Epoch: [63/80], Step: [100/500], Loss: 0.1236\n","Epoch: [63/80], Step: [200/500], Loss: 0.1390\n","Epoch: [63/80], Step: [300/500], Loss: 0.0669\n","Epoch: [63/80], Step: [400/500], Loss: 0.1431\n","Epoch: [63/80], Step: [500/500], Loss: 0.2272\n","Epoch: [64/80], Step: [100/500], Loss: 0.1852\n","Epoch: [64/80], Step: [200/500], Loss: 0.1169\n","Epoch: [64/80], Step: [300/500], Loss: 0.1551\n","Epoch: [64/80], Step: [400/500], Loss: 0.2579\n","Epoch: [64/80], Step: [500/500], Loss: 0.1823\n","Epoch: [65/80], Step: [100/500], Loss: 0.1718\n","Epoch: [65/80], Step: [200/500], Loss: 0.1433\n","Epoch: [65/80], Step: [300/500], Loss: 0.2326\n","Epoch: [65/80], Step: [400/500], Loss: 0.2141\n","Epoch: [65/80], Step: [500/500], Loss: 0.1080\n","Epoch: [66/80], Step: [100/500], Loss: 0.1366\n","Epoch: [66/80], Step: [200/500], Loss: 0.2394\n","Epoch: [66/80], Step: [300/500], Loss: 0.1294\n","Epoch: [66/80], Step: [400/500], Loss: 0.1806\n","Epoch: [66/80], Step: [500/500], Loss: 0.1500\n","Epoch: [67/80], Step: [100/500], Loss: 0.2209\n","Epoch: [67/80], Step: [200/500], Loss: 0.1554\n","Epoch: [67/80], Step: [300/500], Loss: 0.1710\n","Epoch: [67/80], Step: [400/500], Loss: 0.1514\n","Epoch: [67/80], Step: [500/500], Loss: 0.1404\n","Epoch: [68/80], Step: [100/500], Loss: 0.1887\n","Epoch: [68/80], Step: [200/500], Loss: 0.1343\n","Epoch: [68/80], Step: [300/500], Loss: 0.1190\n","Epoch: [68/80], Step: [400/500], Loss: 0.1215\n","Epoch: [68/80], Step: [500/500], Loss: 0.0842\n","Epoch: [69/80], Step: [100/500], Loss: 0.0804\n","Epoch: [69/80], Step: [200/500], Loss: 0.0951\n","Epoch: [69/80], Step: [300/500], Loss: 0.1843\n","Epoch: [69/80], Step: [400/500], Loss: 0.1096\n","Epoch: [69/80], Step: [500/500], Loss: 0.1775\n","Epoch: [70/80], Step: [100/500], Loss: 0.0850\n","Epoch: [70/80], Step: [200/500], Loss: 0.1062\n","Epoch: [70/80], Step: [300/500], Loss: 0.2093\n","Epoch: [70/80], Step: [400/500], Loss: 0.1102\n","Epoch: [70/80], Step: [500/500], Loss: 0.1590\n","Epoch: [71/80], Step: [100/500], Loss: 0.2773\n","Epoch: [71/80], Step: [200/500], Loss: 0.1862\n","Epoch: [71/80], Step: [300/500], Loss: 0.1120\n","Epoch: [71/80], Step: [400/500], Loss: 0.1798\n","Epoch: [71/80], Step: [500/500], Loss: 0.1956\n","Epoch: [72/80], Step: [100/500], Loss: 0.1659\n","Epoch: [72/80], Step: [200/500], Loss: 0.2470\n","Epoch: [72/80], Step: [300/500], Loss: 0.1832\n","Epoch: [72/80], Step: [400/500], Loss: 0.1764\n","Epoch: [72/80], Step: [500/500], Loss: 0.2043\n","Epoch: [73/80], Step: [100/500], Loss: 0.1558\n","Epoch: [73/80], Step: [200/500], Loss: 0.1482\n","Epoch: [73/80], Step: [300/500], Loss: 0.1412\n","Epoch: [73/80], Step: [400/500], Loss: 0.1569\n","Epoch: [73/80], Step: [500/500], Loss: 0.2112\n","Epoch: [74/80], Step: [100/500], Loss: 0.1252\n","Epoch: [74/80], Step: [200/500], Loss: 0.0996\n","Epoch: [74/80], Step: [300/500], Loss: 0.1411\n","Epoch: [74/80], Step: [400/500], Loss: 0.1823\n","Epoch: [74/80], Step: [500/500], Loss: 0.1281\n","Epoch: [75/80], Step: [100/500], Loss: 0.1983\n","Epoch: [75/80], Step: [200/500], Loss: 0.2003\n","Epoch: [75/80], Step: [300/500], Loss: 0.1264\n","Epoch: [75/80], Step: [400/500], Loss: 0.1976\n","Epoch: [75/80], Step: [500/500], Loss: 0.1603\n","Epoch: [76/80], Step: [100/500], Loss: 0.2074\n","Epoch: [76/80], Step: [200/500], Loss: 0.0632\n","Epoch: [76/80], Step: [300/500], Loss: 0.1157\n","Epoch: [76/80], Step: [400/500], Loss: 0.1307\n","Epoch: [76/80], Step: [500/500], Loss: 0.1376\n","Epoch: [77/80], Step: [100/500], Loss: 0.1577\n","Epoch: [77/80], Step: [200/500], Loss: 0.1093\n","Epoch: [77/80], Step: [300/500], Loss: 0.1646\n","Epoch: [77/80], Step: [400/500], Loss: 0.1378\n","Epoch: [77/80], Step: [500/500], Loss: 0.1520\n","Epoch: [78/80], Step: [100/500], Loss: 0.1182\n","Epoch: [78/80], Step: [200/500], Loss: 0.2212\n","Epoch: [78/80], Step: [300/500], Loss: 0.1464\n","Epoch: [78/80], Step: [400/500], Loss: 0.0934\n","Epoch: [78/80], Step: [500/500], Loss: 0.1948\n","Epoch: [79/80], Step: [100/500], Loss: 0.2002\n","Epoch: [79/80], Step: [200/500], Loss: 0.1471\n","Epoch: [79/80], Step: [300/500], Loss: 0.1948\n","Epoch: [79/80], Step: [400/500], Loss: 0.1650\n","Epoch: [79/80], Step: [500/500], Loss: 0.1819\n","Epoch: [80/80], Step: [100/500], Loss: 0.1893\n","Epoch: [80/80], Step: [200/500], Loss: 0.1899\n","Epoch: [80/80], Step: [300/500], Loss: 0.0825\n","Epoch: [80/80], Step: [400/500], Loss: 0.1249\n","Epoch: [80/80], Step: [500/500], Loss: 0.1565\n"]}]},{"cell_type":"markdown","source":["# 3. Test the model"],"metadata":{"id":"IIzsjsjfKOE4"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","  total = 0\n","  correct = 0\n","  for images, labels in test_loader:\n","    # Keep model and data on the same device\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Predict\n","    output = model(images)\n","    _, pred = torch.max(output, dim=1)\n","    \n","    total += labels.size(0)\n","    correct += (pred == labels).sum().item()\n","\n","  print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I64kqk0_jK3H","executionInfo":{"status":"ok","timestamp":1658503808361,"user_tz":-120,"elapsed":1378,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"fd2cea41-48ae-4898-9815-c6c3068f2700"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the model on the test images: 88.6 %\n"]}]}]}