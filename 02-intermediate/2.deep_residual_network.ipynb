{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deep_residual_network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMKyxIsrOB8FlZKII2LX0OG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKKhyxF8Wt0Q"},"outputs":[],"source":["import torch \n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms"]},{"cell_type":"markdown","source":["# 1. Hyper-parameters and Dataset"],"metadata":{"id":"sIbq0KwnJ6jJ"}},{"cell_type":"code","source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","num_epochs = 80\n","batch_size = 100\n","learning_rate = 0.001"],"metadata":{"id":"VCgv7reFW1Fy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIhBOTt4W4Ph","executionInfo":{"status":"ok","timestamp":1658502375524,"user_tz":-120,"elapsed":20942,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"75613d85-c0d1-4cd0-a2ec-3ba5e4db7275"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## 1.1 Image preprocessing\n","\n","**According to the paper:**  [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).\n","1. We follow the simple data augmentation for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip. \n","2. For testing, we only evaluate the single view of the original 32×32 image."],"metadata":{"id":"lQukBxnUaiTe"}},{"cell_type":"code","source":["data_dir = '/content/drive/My Drive/PyTorch/Github_Series/02-intermediate/'\n","\n","# Image preprocessing modules\n","transform = transforms.Compose([\n","  transforms.Pad(padding=4),\n","  transforms.RandomHorizontalFlip(p=0.5),\n","  transforms.RandomCrop(size=32),\n","  transforms.ToTensor()])\n","\n","# CIFAR-10 dataset\n","train_dataset = torchvision.datasets.CIFAR10(root=data_dir,\n","                                             train=True, \n","                                             transform=transform,\n","                                             download=True)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root=data_dir,\n","                                            train=False, \n","                                            transform=transforms.ToTensor())\n","\n","# Data loader\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-66_OAEYtDC","executionInfo":{"status":"ok","timestamp":1658502384110,"user_tz":-120,"elapsed":8589,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"7ddb1801-418d-4ae8-d0af-0352794067d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["# 2. Modeling and Training\n","\n","1. The architecture is illustrated in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf). "],"metadata":{"id":"aOkZn3pwKK9h"}},{"cell_type":"code","source":["# 3x3 convolution\n","def conv3x3(in_channels, out_channels, stride=1):\n","  # padding=1 for keeping the input and output of the same dimensions\n","  return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n","                   stride=stride, padding=1, bias=False)\n","\n","# 1x1 convolution: used to match dimensions\n","def conv1x1(in_channels, out_channels, stride=1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=1, \n","                     stride=stride, bias=False)\n","\n","# Residual block\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","      super().__init__()\n","      self.conv1 = conv3x3(in_channels, out_channels, stride)\n","      # We adopt batch normalization (BN) right after each convolution and before activation\n","      self.bn1 = nn.BatchNorm2d(out_channels)\n","      self.relu = nn.ReLU(inplace=True)\n","      self.conv2 = conv3x3(out_channels, out_channels)\n","      self.bn2 = nn.BatchNorm2d(out_channels)\n","      self.downsample = downsample\n","        \n","    def forward(self, x):\n","      residual = x\n","      out = self.conv1(x)\n","      out = self.bn1(out)\n","      out = self.relu(out)\n","      out = self.conv2(out)\n","      out = self.bn2(out)\n","      if self.downsample:\n","        residual = self.downsample(x)\n","      out += residual\n","      # We adopt the second nonlinearity after the addition, as shown in Figure 5 in the paper.\n","      out = self.relu(out)\n","      return out\n","\n","# ResNet\n","class ResNet(nn.Module):\n","    def __init__(self, layers, num_classes=10):\n","      super().__init__()\n","      self.conv = conv3x3(3, 16)\n","      self.bn = nn.BatchNorm2d(16)\n","      self.relu = nn.ReLU(inplace=True)\n","      # Each layer consists of several Residual blocks\n","      self.layer1 = self.__make_layer(16, 16, layers[0])\n","      self.layer2 = self.__make_layer(16, 32, layers[1], 2)  # stride=2\n","      self.layer3 = self.__make_layer(32, 64, layers[2], 2)\n","      self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","      self.fc = nn.Linear(64, num_classes)\n","\n","    def __make_layer(self, in_channels, out_channels, layer_num, stride=1):\n","      downsample = None\n","      if (stride != 1) or (in_channels != out_channels):\n","        downsample = nn.Sequential(\n","            conv1x1(in_channels, out_channels, stride),\n","            nn.BatchNorm2d(out_channels))\n","      \n","      layers = []\n","      layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n","      for _ in range(1, layer_num):\n","        layers.append(ResidualBlock(out_channels, out_channels))\n","      return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","      out = self.conv(x)\n","      out = self.bn(out)\n","      out = self.relu(out)\n","      out = self.layer1(out)\n","      out = self.layer2(out)\n","      out = self.layer3(out)\n","      out = self.avg_pool(out)\n","      out = torch.flatten(out, start_dim=1)\n","      out = self.fc(out)\n","      return out"],"metadata":{"id":"AbZ_0t6tYvQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize the deep residual model\n","model = ResNet([2, 2, 2]).to(device)\n","\n","# loss and optimizer\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","def update_lr(optimizer, lr):\n","  for param_group in optimizer.param_groups:\n","    param_group['lr'] = lr\n","\n","# Train the model\n","total_step = len(train_loader)\n","curr_lr = learning_rate\n","for epoch in range(num_epochs):\n","  for batch_id, (images, labels) in enumerate(train_loader):\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Feedward\n","    output = model(images)\n","    loss = loss_fn(output, labels)\n","\n","    # Backward propagation\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if (batch_id+1) % 100 == 0:\n","      print('Epoch: [{}/{}], Step: [{}/{}], Loss: {:.4f}'\n","            .format(epoch+1, num_epochs, batch_id+1, total_step, loss.item()))\n","  \n","  # decay learning rate\n","  if (epoch+1) % 20 == 0:\n","    curr_lr /= 3\n","    update_lr(optimizer, curr_lr)"],"metadata":{"id":"TzGCWRShD0w1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Test the model"],"metadata":{"id":"IIzsjsjfKOE4"}},{"cell_type":"code","source":["# Test the model\n","model.eval()\n","with torch.no_grad():\n","  total = 0\n","  correct = 0\n","  for images, labels in test_loader:\n","    # Keep model and data on the same device\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Predict\n","    output = model(images)\n","    _, pred = torch.max(output, dim=1)\n","    \n","    total += labels.size(0)\n","    correct += (pred == labels).sum().item()\n","\n","  print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I64kqk0_jK3H","executionInfo":{"status":"ok","timestamp":1658503808361,"user_tz":-120,"elapsed":1378,"user":{"displayName":"Ji Lan","userId":"06508059979673879762"}},"outputId":"fd2cea41-48ae-4898-9815-c6c3068f2700"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the model on the test images: 88.6 %\n"]}]}]}